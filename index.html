<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs</title>

  <!-- 添加favicon -->
  <!-- <link rel="icon" type="image/x-icon" href="assets/images/icon.png"> -->
  <!-- 或者使用PNG格式 -->
  <link rel="icon" type="image/png" href="assets/images/icon.png">

  <script>
    var task_map = {
      "simple-object-manipulation": "simple_object_manipulation",
      "visual-goal-reaching": "visual_goal_reaching",
      "novel-concept-grounding": "novel_concept_grounding",
      "one-shot-video-imitation": "one_shot_video_imitation",
      "visual-constraint-satisfaction": "visual_constraint_satisfaction",
      "visual-reasoning": "visual_reasoning"
    };

    function updateDemoVideo(category) {
      // var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById(category + "-menu-tasks").value;
      var inst = document.getElementById(category + "-menu-instances").value;

      console.log(task_map[category], task, inst)

      var video = document.getElementById(category + "-single-task-video");
      video.src = "assets/videos/demos/" +
        task_map[category] +
        "/" +
        task +
        "/" +
        inst +
        ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <style>
    .content.has-text-justified p,
    .content.has-text-justified ul,
    .content.has-text-justified ol {
      font-size: 120%;
      /* 可以调整这个百分比来改变大小 */
    }

    .content.has-text-justified li {
      font-size: inherit;
      /* 继承父元素的字体大小 */
    }

    .hover-card p,
    .hover-card ul,
    .hover-card ol {
      font-size: 120%;
    }

    .hover-card li {
      font-size: inherit;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>


  <!-- 
  <d-contents>
    <nav>
      <h4>CONTENTS</h4>
      <div><a href="#Abstract">Abstract</a></div>
      <div><a href="#Method">Method</a></div>
      <div><a href="#Performance">Performance</a></div>
      <div><a href="#Conclusion">Conclusion</a></div>
    </nav>
  </d-contents>
 -->



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode
              Selection and Reasoning Process</h1>
            <!-- <h1 class="title is-2 custom-heading">Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL</h1> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://github.com/xtong-zhang">Xintong Zhang</a><sup>1,2*</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://adaptmmbench.github.io/">Xiaowen Zhang</a><sup>2,3*</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://adaptmmbench.github.io/">Jongrong Wu</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://zhigao2017.github.io/">Zhi Gao</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://adaptmmbench.github.io/">Shilin Yan</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://adaptmmbench.github.io/">Zhenxin Diao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://adaptmmbench.github.io/">Kunpeng Gao</a><sup>2</sup>
              </span>
              <span class="author-block">
                <span class="author-block">
                  <a target="_blank" href="https://adaptmmbench.github.io/">Xuanyan Chen</a><sup>2</sup>
                </span>
                <span class="author-block">
                  <a target="_blank" href="https://wu-yuwei-bit.github.io/">Yuwei Wu</a><sup>1,4†</sup>
                </span>
                <span class="author-block">
                  <a target="_blank" href="https://scholar.google.com/citations?user=Sl6TV7gAAAAJ&hl=en">Yunde
                    Jia</a><sup>4</sup>
                </span>
                <span class="author-block">
                  <a target="_blank" href="https://liqing.io/">Qing Li</a><sup>2†</sup>
                </span>


            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Beijing Key Laboratory of Intelligent Information Technology,
                School of Computer Science & Technology, Beijing Institute of Technology, <sup>2</sup>State Key
                Laboratory of General Artificial Intelligence, BIGAI, <sup>3</sup>School of Intelligence Science and
                Technology, Peking University, <sup>4</sup>Guangdong Laboratory of Machine Perception and Intelligent
                Computing, Shenzhen MSU-BIT University, <sup>5</sup>Department of Automation, Tsinghua University</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution, <sup>†</sup>Corresponding authors</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/abs/2505.15436"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" href="https://github.com/xtong-zhang/Chain-of-Focus"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" href="https://huggingface.co/xintongzhang/CoF-rl-model-7b"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-robot"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <span class="link-block">
                  <a target="_blank" href="https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>


                <img src="assets/images/performance.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto;
                width: 400px;" />

              </div>
            </div>
          </div>
        </div>
  </section>




  <section class="hero is-light" id="Abstract">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3 custom-heading">Abstract</h2>
            <div class="content has-text-justified">
              <!-- <img src="assets/images/teaser.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/> -->
              <p style="font-size: 125%">
                Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs),
                aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance
                both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and
                simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model
                capacities. Consequently, they obscure the distinction between adaptive mode selection and general
                performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a
                comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI,
                knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench
                utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of
                different reasoning modes, isolating this meta-cognition ability by dynamically identifying task
                difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates
                multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational
                efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it
                notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though
                tool effectiveness remains highly inconsistent across model architectures.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>





  <section class="section">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
          <div class="content">
            <table class="js-sort-table" id="results">
              <thead>
                <tr>
                  <th rowspan="2" style="vertical-align: middle;"><strong>#</strong></th>
                  <th rowspan="2" style="vertical-align: middle;"><strong>Model</strong></th>
                  <th colspan="1" style="vertical-align: middle;"><strong>Adaptive Mode Selection</strong>
                  </th>
                  <th colspan="2" style="vertical-align: middle;"><strong>Reasoning Process Quality</strong>
                  </th>
                  <th colspan="3" style="vertical-align: middle;"><strong>Reasoning Process
                      Efficiency</strong></th>
                </tr>
                <tr>
                  <th><strong>MCC</strong></th>
                  <th><strong>Key Step Coverage</strong></th>
                  <th><strong>Tool Effectiveness</strong></th>
                  <th><strong>Step</strong></th>
                  <th><strong>Tool</strong></th>
                  <th><strong>Tokens</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>1</td>
                  <td>GPT-5</td>
                  <td><b>0.41</b></td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td>2</td>
                  <td>Qwen3-vl-235B-Instruct</td>
                  <td>0.26</td>
                  <td><b>84.83</b></td>
                  <td>89.64</td>
                  <td>2.04</td>
                  <td>1.04</td>
                  <td>7531.95</td>
                </tr>
                <tr>
                  <td>3</td>
                  <td>Gemini-3-Pro</td>
                  <td>0.24</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                  <td>-</td>
                </tr>
                <tr>
                  <td>4</td>
                  <td>PyVision</td>
                  <td>0.20</td>
                  <td>77.43</td>
                  <td>62.02</td>
                  <td>2.76</td>
                  <td>1.76</td>
                  <td><b>2481.00</b></td>
                </tr>
                <tr>
                  <td>5</td>
                  <td>AdaptVision</td>
                  <td>0.17</td>
                  <td>72.60</td>
                  <td>81.70</td>
                  <td>1.51</td>
                  <td>0.51</td>
                  <td>4175.96</td>
                </tr>
                <tr>
                  <td>6</td>
                  <td>Qwen3-vl-32B-Instruct</td>
                  <td>0.14</td>
                  <td>83.79</td>
                  <td><b>92.98</b></td>
                  <td>2.42</td>
                  <td>1.44</td>
                  <td>7725.99</td>
                </tr>
                <tr>
                  <td>7</td>
                  <td>PixelReasoner</td>
                  <td>0.11</td>
                  <td>76.02</td>
                  <td>56.51</td>
                  <td>1.37</td>
                  <td>0.37</td>
                  <td>4229.00</td>
                </tr>
                <tr>
                  <td>8</td>
                  <td>Qwen3-vl-8B-Instruct</td>
                  <td>0.06</td>
                  <td>78.40</td>
                  <td>91.62</td>
                  <td>1.76</td>
                  <td>1.20</td>
                  <td>8282.40</td>
                </tr>
                <tr>
                  <td>9</td>
                  <td>Deepeyes v2</td>
                  <td>0.03</td>
                  <td>75.14</td>
                  <td>56.79</td>
                  <td>2.09</td>
                  <td>1.09</td>
                  <td>6918.90</td>
                </tr>
                <tr>
                  <td>10</td>
                  <td>Thyme</td>
                  <td>0.01</td>
                  <td>77.14</td>
                  <td>56.50</td>
                  <td><b>1.05</b></td>
                  <td><b>0.06</b></td>
                  <td>6708.47</td>
                </tr>
                <tr>
                  <td>11</td>
                  <td>Deepeyes</td>
                  <td>0.00</td>
                  <td>75.56</td>
                  <td>50.99</td>
                  <td>2.00</td>
                  <td>1.68</td>
                  <td>7601.45</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section" id="Method">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 custom-heading">Method</h2>

          <div class="content has-text-justified">
            <img src="assets/images/evaluation_strategy.png" class="interpolation-image" alt=""
              style="display: block; margin-left: auto; margin-right: auto" width="900" />
            <p style="font-size: 125%">
              We build a visual search agent based on the ReAct framework, which solves problems iteratively through a
              Thought-Action loop. When executing visual search tasks, the agent employs a specific strategy of
              progressive $\text{zoom-in}$ or refinement via tool execution (e.g., image processing or focusing
              operations). This method enables the agent to acquire more precise, authentic, and valuable search data to
              guide subsequent reasoning and decision steps.
            </p>
          </div>

          <div class="content has-text-justified">
            <!-- <img src="assets/images/model_inference.jpg" class="interpolation-image" alt=""
              style="display: block; margin-left: auto; margin-right: auto" width="900" /> -->
            <div class="content has-text-justified">
              <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                <h3 class="title is-4">Adaptive Mode Selection Evaluation</h3>
                <p>
                  Adaptive intelligence depends on a model’s ability to assess whether its current information is
                  sufficient to solve a task. Consequently, the appropriateness of a reasoning mode should be evaluated
                  independently of answer correctness.
                  Under this principle, the necessity of tool invocation is determined by the outcome of text-only
                  reasoning.
                </p>
                <p>
                  If a task can be solved using text reasoning alone, it is labeled as <strong>Tool-Redundant</strong>,
                  indicating that visual tool invocation is unnecessary and may introduce noise. Conversely, tasks that
                  cannot be solved via text-only reasoning are labeled as <strong>Tool-Required</strong>, indicating
                  that visual tool invocation is necessary to obtain additional information. This categorization defines
                  the mode selection labels used in our evaluation. Accordingly, tool invocation decisions are evaluated
                  using a confusion matrix: TP denotes Tool-Required cases where the model invokes tools, FN denotes
                  Tool-Required cases where the model does not invoke tools, TN denotes Tool-Redundant cases where the
                  model selects text-only reasoning, and FP denotes Tool-Redundant cases where the model unnecessarily
                  invokes tools.
                </p>
                <p><strong>Matthews Correlation Coefficient (MCC).</strong> In adaptive mode selection, the proportions
                  of tool-redundant and tool-required cases are model-dependent, leading to varying degrees of class
                  imbalance in the resulting confusion matrix. To ensure a robust evaluation, we adopt the MCC:</p>

                $$
                \text{MCC} =
                \frac{TP \cdot TN - FP \cdot FN}
                {\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)} + \epsilon}
                $$

                <p>where $\epsilon$ is a small constant for numerical stability. MCC ranges from $[-1,1]$, with $1$
                  indicating perfect agreement with the optimal mode selection, $0$ denoting the chance-level
                  performance, and $-1$ indicating complete misalignment.</p>
              </div>

              <div class="content has-text-justified hover-card" style="padding: 1.5rem; margin-top: 1rem;">
                <h3 class="title is-4">Reasoning Process Evaluation</h3>
                <p>
                  While MCC measures the quality of mode selection, it does not assess the validity of the reasoning
                  process. Models may produce correct answers despite logical errors or improper tool usage. To address
                  this limitation, we introduce three process-oriented metrics to evaluate reasoning coherence and tool
                  execution fidelity.
                </p>
                <p>
                  A reasoning trajectory $\mathcal{R}$ is formalized as an interleaved sequence of reasoning steps and
                  tool invocations:
                  $$ \mathcal{R} = \{(s_1, t_1), (s_2, t_2), \dots, s_n\}, $$
                  where $s_i$ is the reasoning at step $i$ and $t_i \in \mathcal{T}$ represents the corresponding tool
                  invocation. The trajectory terminates at the final reasoning step $s_n$, and produces the answer.
                </p>

                <h4 class="title is-5">Key Steps Coverage</h4>
                <p>
                  Following the evaluation paradigm of MME, we assess whether a model’s reasoning chain
                  $\{s_i\}_{i=1}^n$ covers the essential human-annotated key steps $K$. We employ GPT-5 as an evaluator
                  to identify the presence of these key steps within the generated reasoning, and define the key step
                  coverage as:
                </p>
                $$
                \text{KCoverage}
                =
                \frac{1}{|K|}
                \max_j
                \prod_{i=1}^j
                \mathbb{I}\!\left[k_i \underset{\text{\tiny GPT-5}}{\in} \{s_1,\dots,s_n\}\right].
                $$
                <p>
                  This metric measures how far the model’s reasoning progresses along the key steps. Rather than
                  penalizing skipped or compressed steps, KCoverage captures the maximum extent to which the reasoning
                  aligns with the solution structure, allowing different reasoning styles and reflecting how close the
                  model comes to a correct solution.
                </p>

                <h4 class="title is-5">Tool Execution Effectiveness</h4>
                <p>
                  To assess the precision of tool usage, we evaluate whether each tool invocation is semantically
                  appropriate for its corresponding reasoning step and free of execution errors. The tool effectiveness
                  is defined as:
                </p>
                $$
                \text{Effect}_{\text{tool}}
                =
                \frac{1}{N_{\text{tool}}}
                \sum_{i=1}^{N_{\text{tool}}}
                \text{valid}_{\text{GPT-5}}(t_i \mid s_i),
                $$
                <p>
                  where $N_{\text{tool}}$ denotes the total number of tool invocations, $t_i \in \mathcal{T}$ is the
                  tool invoked at step $i$, and $\text{valid}_{\text{GPT-5}}(\cdot) \in \{0,1\}$ is a semantic validity
                  judgment provided by GPT-5.
                </p>
              </div>
            </div>
          </div>
        </div>
  </section>

  <section class="section" id="AdaptMMBench Benchmark">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 custom-heading">AdaptMMBench</h2>

          <div class="content has-text-justified">
            <img src="assets/images/overview_data.png" class="interpolation-image" alt=""
              style="display: block; margin-left: auto; margin-right: auto" width="900" />
            <p style="font-size: 125%">
              AdaptMMBench encompasses 1,420 samples spanning five domains: real-world, OCR, GUI, math, and knowledge.
              To ensure a comprehensive evaluation, the dataset includes diverse difficulty levels, balancing tasks
              solvable via text-only reasoning with those requiring adaptive tool invocation, such as zoom-in and
              geometric transformations.
              Benchmark quality is ensured through a rigorous multi-stage verification pipeline, where human annotators
              and GPT-5 collaborate to verify ground truth and key reasoning steps.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!-- <hr style="width: 60%; border-top: 1px solid #e0e0e0; margin: 40px auto;"> -->

  <section class="section" id="Performance">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 custom-heading">Performance</h2>

          <div class="columns is-centered is-vcentered">
            <div class="column is-6 has-text-centered">
              <img src="assets/images/error_analysis.png" class="interpolation-image" alt="Error Analysis"
                style="width: 100%;" />
            </div>
            <div class="column is-6">
              <div class="content has-text-justified">
                <p>
                  In this section, we analyze the causes of incorrect predictions made by GPT-5 under the adaptive mode
                  to understand the gap between adaptive reasoning and oracle-visual mode. As shown in the figure, most
                  errors are related to tool usage. Specifically, 42.3% of the errors stem from visual reasoning
                  failures, such as zoom-in into incorrect regions or applying wrong image transformations. Another 7.3%
                  of errors occur even when visual reasoning is correct. Since these samples are solvable in the
                  oracle-visual mode, this suggests that intermediate images in multi-step reasoning may introduce
                  visual noise affecting the final prediction. In addition, 8.3% of errors are caused by incorrect mode
                  selection, where text reasoning is sufficient but the model unnecessarily invokes tools, leading to
                  degraded performance. For cases without tool usage, forcing tool invocation corrects 7.0% of the
                  errors, while 6.3% remain incorrect. The remaining 28.8% of errors exceed the capability of the GPT-5
                  model.
                </p>
              </div>
            </div>
          </div>

          <div class="column is-full has-text-centered content">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/images/figure10.png" alt="" width="100%" />
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/images/figure11.png" alt="" width="100%" />
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/images/figure13.png" alt="" width="100%" />
                </div>
              </div>
            </div>
          </div>
          <hr class="section-line">
          <h3 class="subsection-title">Adaptive Case Study</h3>
          <img src="assets/images/adaptive_case_2.png" class="interpolation-image" alt=""
            style="display: block; margin-left: auto; margin-right: auto; width: 100%; margin-bottom: 1rem;" />
          <img src="assets/images/adaptive_case_1.png" class="interpolation-image" alt=""
            style="display: block; margin-left: auto; margin-right: auto; width: 100%;" />


          <!-- <div class="content has-text-justified"> -->

        </div>
      </div>
    </div>
  </section>




  <section class="hero is-light" id="Conclusion">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Generalization Videos -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3 custom-heading">Conclusion</h2>
            <div class="column">
              <div class="content has-text-justified">
                <p style="font-size: 125%">
                  In this paper, we present adaptive chain-of-focus (Adaptive-CoF), a framework that enables
                  VLMs to
                  adaptively perform fine-grained visual search and zooming. Through a two-stage training
                  pipeline
                  combining supervised fine-tuning and reinforcement learning, Adaptive-CoF learns to balance
                  detailed
                  perception with computational efficiency, overcoming the trade-off between static viewing
                  and
                  exhaustive zooming. Experiments demonstrate state-of-the-art performance on challenging
                  benchmarks
                  with significantly reduced computational cost. </p>

              </div>
            </div>

          </div>
        </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@misc{zhang2025chain,
      title={Adaptive Chain-of-Focus Reasoning via Dynamic Visual Searchand Zooming for Efficient VLMs}, 
      author={Xintong Zhang and Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaowen Zhang and Yang Liu and Tao Yuan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li},
      year={2025},
      eprint={2505.15436},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.15436},  -->
      <pre><code></code>@article{zhang2025chain,
      title={Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs},
      author = {Zhang, Xintong and Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Zhang, Xiaowen and Liu, Yang and Yuan, Tao and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and Qing Li},
      journal={arXiv preprint arXiv:2505.15436},
      year={2025}
      }

 </code></pre>
    </div>
  </section>


  <!-- 
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

  <script src="static/js/contents_bar.js"></script>


</body>

</html>