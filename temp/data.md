\subsection{Data Collection}

\bench\ encompasses 1,420 samples spanning five domains: real-world, OCR, GUI, math, and knowledge, enabling a comprehensive evaluation of adaptive reasoning across diverse scenarios, as detailed in Fig.~\ref{fig:bench_domains}.

To ensure that \bench\ contains both samples solvable via text-only reasoning and samples that require visual tool invocation under adaptive reasoning, we deliberately construct the dataset with diverse difficulty levels during data collection. One subset consists of samples solved by Qwen2.5-VL-7B under text-only reasoning. A second subset includes samples that Qwen2.5-VL-7B fails but can be solved by Qwen3-VL-235B based on adaptive reasoning. A small portion remains unsolved even by Qwen3-VL-235B. The relative proportions of these three subsets are approximately 24\%, 70\%, and 6\%. Notably, these subsets are introduced only to ensure difficulty diversity and do not determine the ground truth reasoning mode during evaluation. The reasoning mode selection label in adaptive mode is made by the model itself, as detailed in Sec.~\ref{sec:metrics}.

Building on prior adaptive reasoning methods~\cite{chern2025thinking, zhang2025thyme, zhao2025pyvision}, \bench\ evaluates diverse visual tools beyond zoom-in, including geometric transformations for orientation correction and photometric adjustments for visual enhancement. During data construction, these requirements are induced via controlled distortions such as changes in contrast, brightness, and orientation, with zoom-in and transformation samples with a ratio of 5:2. We further include 120 samples requiring auxiliary-line generation, suggesting that reasoning with self-generated images constitutes an important extension of the think-with-images paradigm.

\subsection{Annotation and Quality Control}

We collect initial data from existing benchmarks, with annotators providing bounding-box annotations for key regions, while visual enhancement annotations are generated through predefined transformations. Distortion parameters are constrained to maintain recoverability. GPT-5 is used to generate key reasoning steps $K$, which are manually verified. These components form annotated quintuples $(I, Q, A, E, K)$.

\noindent\textbf{Quality Control.}
Benchmark quality is ensured through a multi-stage verification pipeline. First, three independent annotators cross-validate each QA pair to remove ambiguity and verify correctness. Annotated image transformations and generated key reasoning steps are then reviewed by additional annotators for precision. Inaccurate instances are iteratively refined or re-annotated. This process ensures high-fidelity ground truth with precise pixel-level annotations and reliable key reasoning steps for comprehensively evaluating adaptive reasoning. More statistical information of \bench\ can be found in Appendix~\ref{sec:Data_Source_Distribution}.